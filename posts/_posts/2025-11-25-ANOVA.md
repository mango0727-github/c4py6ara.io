---
layout: post
title: ANalysis Of VAriance
description: >
  This post is to introduce one of powerful statistical methods, ANOVA. 
sitemap: false
math: true
hide_last_modified: true
---

Estimation and hypothesis testing have been playing as main pillars in statistical inference in that they provide solid statistical reasoning based on samples collected from populations of interest. Over decades, various testing methodologies have been invented by renowned data scientists and engineers. Among the methodologies, a t-test is a simple but powerful tool in case where a population is assumed to be normally distributed and the variance is unknown. Also, thanks to central limit theorem, T-test is widely utilized. However, the use of the test is only limited to cases where only one or two populations exist, therefore we need another tool to work with populations more than two, which is known as the analysis of variance or ANOVA.

The idea is simple. A t-test is used to compare the means of two populations (or groups) when the population variances are unknown. 
> Yeah, under this setting, there are two common cases: one where we assume equal variances (homoscedasticity) across populations, and one where we do not (think of Welchâ€™s t-test). However, we will not go into these details here, since our focus is ANOVA.

We utilize T-statistic as a test statistic for the T-test.

ANOVA is typically used when we want to compare three or more population means at the same time, although it can also be applied to two groups.

> Fun fact: when ANOVA is applied to only two groups, it gives exactly the same result as the t-test (because $F = t^2$ ). We can put like this (hopefully not too simple): very roughly speaking, if the variance between groups is large compared to the variance within groups, then the populations are likely to have different means.

All in all, the inference is based on samples collected from the populations, and we are trying to test whether all population means are equal or at least one of them is different. We assume that each population is normally distributed with a common variance. Under this model, the F-statistic is defined as:

$$ F = \frac{\text{mean of squared deviations between groups}}{\text{mean of squared deviations within groups}} = \frac{
  \dfrac{1}{k-1}\displaystyle\sum_{i=1}^k n_i \left(\bar x_i - \bar x_{\cdot\cdot}\right)^2
}{
  \dfrac{1}{N-k}\displaystyle\sum_{i=1}^k \sum_{j=1}^{n_i} \left(x_{ij} - \bar x_i\right)^2
} $$

where $k-1$ is the degree-of-freedom for between-group variance, and $N-k$ is the degree-of-freedom for within-group variance. $N$ is the number of samples, and $k$ is the number of groups. The F-statistic follows an F-distribution under the null hypothesis. 
