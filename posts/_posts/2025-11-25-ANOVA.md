---
layout: post
title: ANalysis Of VAriance
description: >
  This post is to introduce one of powerful statistical methods, ANOVA. 
sitemap: false
math: true
hide_last_modified: true
---


## Overview

Estimation and hypothesis testing have been playing as main pillars in statistical inference in that they provide solid statistical reasoning based on samples collected from populations of interest. Over decades, various testing methodologies have been invented by renowned data scientists and engineers. Among the methodologies, a t-test is a simple but powerful tool in case when the population is assumed to be normally distributed and the variance is unknown. In practice, thanks to the central limit theorem, the t-test is also quite robust and widely used even when the normality assumption is only approximately satisfied. However, the t-test is essentially designed for comparing one or two population means. When we want to compare more than two populations while controlling the overall Type I error, we use analysis of variance (ANOVA).

### T-Test

About t-test briefly, the idea is simple. t-test is used to compare the means of two populations (or groups) when the population variances are unknown. 
> Yeah, under this setting, there are two common cases: one where we assume equal variances (homoscedasticity) across populations, and one where we do not (think of Welchâ€™s t-test). However, we will not go into these details here, since our focus is ANOVA.

We utilize T-statistic as a test statistic for the T-test. T-statistic is defined as:

$$ T = \frac{ \bar{X}  }{ s / \sqrt{n} }
$$

## ANOVA

On the other hand, ANOVA is typically used when we want to compare three or more population means at the same time, although it can also be applied to two groups.

> Fun fact: when ANOVA is applied to only two groups, it gives exactly the same result as the t-test (because $F = t^2$ ). We can put like this (hopefully not too simple): very roughly speaking, if the variance between groups is large compared to the variance within groups, then the populations are likely to have different means.

### F-Distribution

Before we go deep into ANOVA, we should know F-distribution. The probability distribution is also known as a ratio's distribution, where the ratio is defined as a fraction of means of two statistics that follow the chi-square distribution. A statistic that follows F-distribution is defined as:

$$ F = \frac{
  \displaystyle V_1 / d_1
}{
  \displaystyle V_2 / d_2
} $$

where $V_1$ and $V_2$ are statistics that follow the chi-square distribution with $d_1$ and $d_2$ degree-of-freedom, respectively. $V_1$ and $V_2$ are assumed to be independent with each other. q

There are many notable properties associated with F-distribution, however we narrow down to a property which is the relationship between a F-statistic and the chi-square distribution.

Since a random variable (RV) following the chi-square distribution is the sum of independent standard normal random variables, the F-statistic above can be written as:

$$
F
= \frac{V_1/d_1}{V_2/d_2}
= \frac{\dfrac{1}{d_1}\sum_{i=1}^{d_1}\dfrac{X_i^2}{\sigma_x^2}}{\dfrac{V_2}{d_2}}
$$


All in all, the inference is based on samples collected from the populations, and we are trying to test whether all population means are equal or at least one of them is different. We assume that each population is normally distributed with a common variance. Under this model, the F-statistic is defined as:

$$ F = \frac{\text{mean of squared deviations between groups}}{\text{mean of squared deviations within groups}} = \frac{
  \dfrac{1}{k-1}\displaystyle\sum_{i=1}^k n_i \left(\bar x_i - \bar x_{\cdot\cdot}\right)^2
}{
  \dfrac{1}{N-k}\displaystyle\sum_{i=1}^k \sum_{j=1}^{n_i} \left(x_{ij} - \bar x_i\right)^2
} $$

where \(k - 1\) is the degree-of-freedom for between-group variance, and $N - k$ is the degree-of-freedom for within-group variance. $N$ is the number of samples, and $k$ is the number of groups. The F-statistic follows an F-distribution under the null hypothesis.
